# -*- coding: utf-8 -*-
"""python_b2_proyecto_final - assigment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14gzsEJ46F64nM5WaRkBGNi53Sdlrc7_I

*Por favor, imagina que estás trabajando en un proyecto como **Científico de Datos** y se te solicita completar la siguiente información.*

## **Información del estudiante**
---
* **Título**: Proyecto Final - B2
* **Autor**: Carlos Yañez Puig
* **Correo**: cyanezu@uoc.edu
* **Fecha**: 14/01/2025
* **Salida**: ipynb, predicciones.csv
---

### **Contexto**:
Este ejercicio se basa en el conjunto de datos GFT Open Finance, cuyo objetivo es fomentar la competencia entre proveedores financieros, impulsar la innovación digital y promover nuevos servicios basados en datos. Este conjunto de datos se centra en la banca abierta y ofrece una valiosa oportunidad de aprendizaje sobre datos y tecnologías abiertas en el sector financiero.

La actividad propuesta permitirá a los estudiantes enfrentar la nueva realidad del intercambio de información bancaria, confrontando bases de datos de diferentes instituciones financieras, incluidas dos bancos y una compañía de seguros.

El objetivo es practicar habilidades de ingeniería de datos y ciencia de datos al desarrollar un sistema que sugiera **la clasificación de los tipos de financiamiento para un cliente específico**. Esto contribuirá a mejorar la experiencia del cliente mediante el diseño de modelos que optimicen la oferta de tipos de financiamiento.

Open Finance representa la evolución de Open Banking y promete traer más transparencia y autonomía a los usuarios del sistema financiero. Los datos proporcionados corresponden a la implementación de Open Finance en Brasil, donde se permite el intercambio de datos sobre seguros, inversiones y fondos de pensiones. El conjunto de datos incluye información financiera de un banco minorista (RetailBankEFG) obtenida a través de financiamiento abierto, con el consentimiento de los clientes, así como datos de un banco de inversión (InvestmentBankCDE) y una compañía de seguros (InsuranceCompanyABC). Estos conjuntos de datos contienen datos de compras anteriores de clientes, así como algunos datos demográficos.

### **Problema:**
El reto consiste en concebir una solución que integre las bases de datos de estas instituciones financieras en el contexto emergente de Open Finance, procese los datos con eficacia y cree modelos de clasificación para tipos de financiamientos utilizando algoritmos de aprendizaje automático.

### **Preguntas:**
Durante el desarrollo del ejercicio encontrarás una o más secciones de preguntas. Por favor, responde de manera justificada y, si se solicita incluir código, investiga para responder adecuadamente.

**Nota:** Para algunas respuestas, debes proporcionar la solución mediante código en Python e implementarla justo debajo de la línea correspondiente.
`#Write your code here`

### **Restricciones:**
No puedes borrar los nombres de las variables propuestas para el desarrollo del ejercicio.

### **Actividades:**
Para elaborar una solución que consolide las bases de datos de estas instituciones financieras y genere modelos de clasificación de tipos de financiamiento mediante algoritmos de aprendizaje automático, podemos dividir el proceso en los siguientes pasos:

- **Preparación de datos:** Integrar y limpiar los datos de las bases de datos de las instituciones financieras, asegurando que estén en un formato adecuado para su análisis.
- **Exploración de datos:** Realizar un análisis exploratorio de los datos para comprender mejor su estructura, distribución y características. Esto puede incluir la visualización de datos y la identificación de posibles patrones o relaciones.
- **Ingesta de datos:** Integrar las bases de datos de las instituciones financieras en un único repositorio de datos, asegurando que la información se pueda acceder de manera eficiente y segura.
- **Procesamiento de datos:** Realizar transformaciones adicionales en los datos según sea necesario, como la codificación de variables categóricas, la normalización de características numéricas y la manipulación de valores faltantes.
- **Desarrollo de modelos para clasificar los tipos de financiamiento:** Seleccionar y entrenar algoritmos de clasificación adecuados para el problema de clasificación de tipos de financiamiento. Esto puede incluir técnicas como algoritmos de clasificación supervisada, tales como Support Vector Machines, Random Forests o redes neuronales.
- **Validación del modelo:** Evaluar el rendimiento de los modelos utilizando métricas adecuadas, como precisión, recall, F1-score, etc. Utilizar técnicas como la validación cruzada para garantizar la generalización del modelo.
- **Optimización del modelo:** Ajustar hiperparámetros y realizar selección de características para optimizar el rendimiento de los modelos.

## **Solución**:

## **Entorno**:

## **Origen de la fuente de datos**:

Los datos están ubicados en la carpeta "data" y constan de los siguientes archivos.

* InvestmentBankCDE.csv
* RetailBankEFG.csv
* InsuranceCompanyABC.csv

Nota: Los datos proporcionados son ficticios y no se corresponden con la realidad de ninguna manera. A continuación, se muestran algunas de las columnas a utilizar por el modelo.

*Por favor, agrega aquellas columnas que faltan y que se encuentran en el archivo InsuranceCompanyABC.csv*




```python
[
  "seguro auto",
  "seguro vida Emp",
  "seguro vida PF",
  "Seguro Residencial",
  "Investimento Fundos_cambiais",
  "Investimento Fundos_commodities",
  "Investimento LCI",
  "Investimento LCA",
  "Investimento Poupanca",
  "Investimento Fundos Multimercado",
  "Investimento Tesouro Direto",
  "Financiamento Casa",
  "Financiamento Carro",
  "Emprestimo _pessoal",
  "Emprestimo _consignado",
  "Emprestimo _limite_especial",
  "Emprestimo _educacao",
  "Emprestimo _viagem",
  "Investimento CDB",
  "Investimento Fundos"
]
```

# Librerías
Las siguientes son varias de las librerias necesarias para el desarrollo del ejercicio; sin embargo, estas no estan limitadas es decir puedes incluir otras librerias para desarrollar el ejercico.
"""

import re
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.ensemble import (
    RandomForestClassifier,
    VotingClassifier,
    ExtraTreesClassifier,
    AdaBoostClassifier,
    GradientBoostingClassifier,
)
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import RidgeClassifier, LogisticRegression
from sklearn.model_selection import (
    train_test_split,
    learning_curve,
    ShuffleSplit,
    cross_val_score,
    KFold,
)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from functools import reduce
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE, RandomOverSampler, SMOTENC, ADASYN
from imblearn.under_sampling import RandomUnderSampler, NearMiss
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import itertools
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedShuffleSplit

"""## Configuración de visualización de conjuntos de datos"""

pd.set_option("display.max_columns", None)
pd.set_option("display.max_colwidth", None)

"""### Descarga las fuentes de datos

Si estás utilizando Google Colaboratory o un entorno Linux con la herramienta wget, puedes descomentar las siguientes líneas para descargar los datos.
"""

#!mkdir data
#!wget https://raw.githubusercontent.com/maratonadev/desafio-3-2021/main/assets/data/InvestmentBankCDE.csv  -O data/InvestmentBankCDE.csv
#!wget https://raw.githubusercontent.com/maratonadev/desafio-3-2021/main/assets/data/RetailBankEFG.csv -O data/RetailBankEFG.csv
#!wget https://raw.githubusercontent.com/maratonadev/desafio-3-2021/main/assets/data/InsuranceCompanyABC.csv -O data/InsuranceCompanyABC.csv

"""## Variables
No puedes borrar las variables descritas a continuación; sin embargo, puedes incluir tus propias variables si estas te ayudan a responder alguna pregunta.
"""

df_retailbank = None
df_investment = None
df_insurance = None
data_frame_merged = None
data_frame_tipo_financiamiento = None
tipo_financiamiento_mapping = None
pca_model = None
X_principal = None

"""## Funciones
A continuación se presentan algunas funciones para gráficar que pueden ser útiles.
"""


def plot_boxplot_violinplot(x, y, data_frame):
    """
    Plot both boxplot and violinplot for comparison.

    Parameters:
    x (str): The column name for the x-axis.
    y (str): The column name for the y-axis.
    data_frame (pandas.DataFrame): The DataFrame containing the data.

    Returns:
    None
    """
    # Set figure size
    plt.figure(figsize=(10, 8))

    # Create subplots
    fig, axes = plt.subplots(2, 1)

    # Rotate x-axis labels
    for ax in axes:
        ax.tick_params(axis="x", rotation=70)

    # Plot violinplot
    sns.violinplot(data=data_frame, x=x, y=y, ax=axes[0])

    # Plot boxplot
    sns.boxplot(data=data_frame, x=x, y=y, ax=axes[1])

    # Adjust layout
    plt.tight_layout()

    # Show plots
    plt.show()


def plot_count_plots(df_base, columnas):
    """
    Plot count plots for specified columns.

    Parameters:
    df_base (pandas.DataFrame): The DataFrame containing the data.
    columnas (list): A list of column names to plot.

    Returns:
    None
    """
    # Determine the number of rows and columns for subplots
    num_plots = len(columnas)
    num_rows = (num_plots + 1) // 2  # Ensure there's at least one row
    num_cols = min(2, num_plots)  # Maximum of 2 columns

    # Create subplots
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))

    # Flatten axes if necessary
    if num_plots == 1:
        axes = [axes]
    else:
        axes = axes.flatten()

    # Iterate through each column
    for columna, ax in zip(columnas, axes):
        # Plot count plot
        sns.countplot(x=columna, data=df_base, ax=ax)
        ax.set_title(f"Count Plot for {columna}")  # Add title
        ax.tick_params(axis="x", rotation=90)  # Rotate x-axis labels

    # Adjust layout
    plt.tight_layout()


def plot_confusion_matrix(
    cm, mapping, title="Confusion matrix", cmap=None, normalize=True
):
    # Calculate accuracy and misclassification rate
    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    # Set default color map if not provided
    if cmap is None:
        cmap = plt.get_cmap("Blues")

    # Create a new figure
    plt.figure(figsize=(8, 6))

    # Display confusion matrix as image
    plt.imshow(cm, interpolation="nearest", cmap=cmap)
    plt.title(title)
    plt.colorbar()
    labes = [mapping[key] for key in mapping.keys()]
    print("mapping", mapping)
    # Display target names on ticks if provided
    if labes is not None:
        tick_marks = np.arange(len(labes))
        plt.xticks(tick_marks, labes, rotation=45)
        plt.yticks(tick_marks, labes)

    # Normalize confusion matrix if required
    if normalize:
        cm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]

    # Set threshold for text color based on normalization
    thresh = cm.max() / 1.5 if normalize else cm.max() / 2

    # Add text annotations to cells
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(
                j,
                i,
                "{:0.2f}".format(cm[i, j]),
                horizontalalignment="center",
                color="white" if cm[i, j] > thresh else "black",
            )
        else:
            plt.text(
                j,
                i,
                "{:,}".format(cm[i, j]),
                horizontalalignment="center",
                color="white" if cm[i, j] > thresh else "black",
            )

    # Set labels for axes
    plt.tight_layout()
    plt.ylabel("True label")
    plt.xlabel(
        "Predicted label\naccuracy={:0.4f}; misclass={:0.4f}".format(accuracy, misclass)
    )

    # Show the plot
    plt.show()


def plot_accuracy_scores(
    estimator, train_x, train_y, test_x, test_y, nparts=5, jobs=None
):
    # Initialize KFold with specified number of splits, shuffling, and random state
    kfold = KFold(n_splits=nparts, shuffle=True, random_state=123)

    # Create a new figure and axes for the plot
    fig, axes = plt.subplots(figsize=(7, 3))

    # Set plot title and labels for x and y axes
    axes.set_title("Accuracy Ratio / Fold Number")
    axes.set_xlabel("Fold Number")
    axes.set_ylabel("Accuracy")

    # Compute accuracy scores for training data using cross-validation
    train_scores = cross_val_score(
        estimator, train_x, train_y, cv=kfold, n_jobs=jobs, scoring="accuracy"
    )

    # Compute accuracy scores for test data using cross-validation
    test_scores = cross_val_score(
        estimator, test_x, test_y, cv=kfold, n_jobs=jobs, scoring="accuracy"
    )

    # Generate sequence of fold numbers
    train_sizes = range(1, nparts + 1, 1)

    # Add grid lines to the plot
    axes.grid()

    # Plot accuracy scores for training data
    axes.plot(train_sizes, train_scores, "o-", color="r", label="Training Data")

    # Plot accuracy scores for cross-validation data
    axes.plot(train_sizes, test_scores, "o-", color="g", label="Cross-Validation")

    # Add legend to the plot
    axes.legend(loc="best")

    # Return the accuracy scores for training data
    return train_scores


def startified_train_test_split(X, Y, n_splits=1, test_size=0.2, random_state=42):
    # Assuming X and y are your feature matrix and target variable respectively

    # Initialize StratifiedShuffleSplit
    stratified_splitter = StratifiedShuffleSplit(
        n_splits=n_splits, test_size=test_size, random_state=random_state
    )

    # Split the data while maintaining the class distribution
    for train_index, test_index in stratified_splitter.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    return X_train, X_test, y_train, y_test


def plot_pca_cumulative_variance(pca):
    """
    Plot the cumulative explained variance of principal components.

    Parameters:
    pca (PCA): The fitted PCA object.

    Returns:
    None
    """
    # Determine explained variance using explained_variance_ration_ attribute
    exp_var_pca = pca.explained_variance_ratio_

    # Cumulative sum of eigenvalues; This will be used to create step plot
    # for visualizing the variance explained by each principal component.
    cum_sum_eigenvalues = np.cumsum(exp_var_pca)

    # Create the visualization plot
    plt.bar(
        range(1, len(exp_var_pca) + 1),
        exp_var_pca,
        alpha=0.5,
        align="center",
        label="Individual explained variance",
    )
    plt.step(
        range(1, len(cum_sum_eigenvalues) + 1),
        cum_sum_eigenvalues,
        where="mid",
        label="Cumulative explained variance",
    )
    plt.ylabel("Explained variance ratio")
    plt.xlabel("Principal component index")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.show()


def get_pca_components(pca, columns):
    # Number of components
    n_pcs = pca.components_.shape[0]

    # Get the index of the most important feature on EACH component i.e. largest absolute value
    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]

    # Initial feature names
    initial_feature_names = columns

    # Get the names
    most_important_names = [
        initial_feature_names[most_important[i]] for i in range(n_pcs)
    ]

    # Using dictionary comprehension to create a dictionary
    dic = {"PC{}".format(i + 1): most_important_names[i] for i in range(n_pcs)}

    # Return a DataFrame sorted by keys
    return pd.DataFrame(sorted(dic.items()))


def plot_elbow_curve_pca(X_principal):
    """
    Plot the elbow curve for PCA.

    Parameters:
    X_principal (DataFrame): Transformed features into principal components.

    Returns:
    None
    """
    ks = range(1, 10)
    inertias = []

    for k in ks:
        # Create a KMeans instance with k clusters: model
        model = KMeans(n_clusters=k, n_init="auto")

        # Fit model to samples
        model.fit(X_principal)

        # Append the inertia to the list of inertias
        inertias.append(model.inertia_)

    # Plot ks vs inertias
    plt.plot(ks, inertias, "-o")
    plt.xlabel("Number of clusters, k")
    plt.ylabel("Inertia")
    plt.xticks(ks)
    plt.show()


"""# **Pregunta 1**

# Preparación de datos

La preparación de datos es un paso crucial en el proceso de análisis de datos. Asegura que los datos sean precisos, consistentes y utilizables para análisis posteriores. A continuación, se presentan los pasos que vamos a seguir para realizar una limpieza de datos efectiva:

1. **Entender los Datos**
   - **Recopilar Información**: Conocer la fuente de los datos, cómo se recolectaron y su estructura.
   - **Exploración Inicial**: Examinar los datos para entender su contenido, formato y posibles problemas.

2. **Evaluación de Calidad de Datos**
   - **Valores Faltantes:** Identificar valores nulos o faltantes en el conjunto de datos.
   - **Duplicados:** Detectar filas duplicadas que pueden distorsionar los análisis.
   - **Inconsistencias:** Buscar inconsistencias en los datos, como diferentes formatos de fechas o variaciones en la nomenclatura.

3. **Ingeniería de características:**
   - **Transformaciones:** Muchas técnicas de ingeniería de características, como la creación de términos de interacción, características polinómicas o agregaciones, son más significativas e interpretables en la escala original de los datos.

4. **Limpieza de Datos**
   - **Manejo de Valores Faltantes**
      - **Eliminar:** Remover filas o columnas con valores faltantes si son pocas y no impactan el análisis.
      - **Imputar:** Rellenar valores faltantes usando métodos como la media, mediana, moda o técnicas más avanzadas como la imputación con modelos predictivos.
   - **Eliminación de Duplicados**
      - **Identificar y Eliminar:** Usar herramientas para detectar y remover filas duplicadas.
   - **Corrección de Inconsistencias**
      - **Estándar de Formato:** Uniformizar formatos de fechas, texto, etc.
      - **Reemplazar Valores Erróneos:** Corregir errores tipográficos y valores fuera de rango.
   - **Normalización y Escalado**
      - **Normalización:** Convertir datos a una escala común.
      - **Escalado:** Ajustar los valores para que estén dentro de un rango específico, útil para algoritmos de machine learning.

# Entender los Datos
##Recopilar Información
## Preguntas
* *¿Cuáles son los desafíos clave al integrar y analizar datos de diferentes instituciones financieras para desarrollar sistemas de recomendación de seguros?*

    Uno de los principales desafíos es la heterogeneidad de los datos, ya que cada institución
    financiera puede utilizar distintos formatos, estructuras y criterios para almacenar la
    información de los clientes. Esto obliga a realizar un proceso de normalización y limpieza
    previo al análisis.

    Otro desafío importante es la calidad de los datos, ya que pueden existir valores faltantes,
    duplicados o inconsistencias que afecten al rendimiento de los modelos de machine learning.

    Además, la integración de datos requiere identificar correctamente un atributo común que
    permita relacionar los registros de un mismo cliente entre distintas entidades, evitando
    errores de emparejamiento.

    Por último, también es relevante considerar aspectos de privacidad y seguridad, asegurando
    que el tratamiento de los datos se realice de forma responsable y conforme al consentimiento
    del usuario.

* *¿De qué manera podría su participación en el desarrollo de nuevas fuentes de información de seguros en el marco de Open Finance promover la transparencia y autonomía de los usuarios del sistema financiero?*

    La participación en el desarrollo de nuevas fuentes de información de seguros dentro del
    marco de Open Finance permite que los usuarios tengan un mayor control sobre sus propios
    datos financieros.

    Al centralizar y compartir información de forma estandarizada, los usuarios pueden comparar
    productos de distintas entidades, entender mejor sus opciones y tomar decisiones más
    informadas.

    Esto promueve la transparencia del sistema financiero y reduce la dependencia de una única
    institución, fomentando la competencia y la innovación en los servicios ofrecidos.

* *¿Cuál es la similitud entre Open Finance y otras fuentes de datos financieros abiertos, como Open Banking y Open Insurance, y cómo benefician a los usuarios del sistema financiero en términos de transparencia y acceso a información?*

    Open Finance, Open Banking y Open Insurance comparten el objetivo de permitir el acceso y
    la portabilidad de los datos financieros de los usuarios entre diferentes proveedores,
    siempre con su consentimiento.

    Estas iniciativas se basan en el uso de estándares abiertos y APIs para facilitar el
    intercambio de información de forma segura y transparente.

    Gracias a ello, los usuarios obtienen una visión más completa de su situación financiera,
    pueden acceder a mejores productos y servicios, y se incrementa la competencia entre las
    entidades financieras.

* *¿Qué aspectos clave deberías revisar al explorar los datos de GFT Open Finance para entender su contenido, formato y posibles problemas, y cómo estos podrían afectar el desarrollo de modelos de machine learning para recomendaciones de seguros?*

    Al explorar los datos es fundamental revisar la estructura del dataset, el significado de
    cada variable y los tipos de datos asociados a cada columna.

    También es importante analizar la presencia de valores nulos, duplicados o inconsistencias,
    ya que estos pueden introducir sesgos o errores en los modelos de machine learning.

    Otro aspecto clave es estudiar la distribución de las variables y posibles desbalances entre
    clases, lo que puede afectar al rendimiento de los modelos de clasificación.

    Una correcta exploración permite tomar decisiones informadas sobre la limpieza, el
    preprocesamiento y la selección de características necesarias para desarrollar modelos de
    recomendación fiables.

## Exploración Inicial

Comencemos importando los diferentes conjuntos de datos como dataframes utilizando la librería de pandas. Luego, procederemos a presentar los primeros 10 registros.
"""

# Write your code here
df_retailbank = pd.read_csv("data/RetailBankEFG.csv")
print("RetailBankEFG - First 10 records")
print(df_retailbank.head(10))

"""*Realiza la misma acción para InvestmentBankCDE.csv.*"""

# Write your code here
df_investment = pd.read_csv("data/InvestmentBankCDE.csv")
print("InvestmentBankCDE - First 10 records")
print(df_investment.head(10))

"""*Realiza la misma acción para InvestmentBankCDE.csv.*"""

# Write your code here
df_insurance = pd.read_csv("data/InsuranceCompanyABC.csv")
print("InsuranceCompanyABC - First 10 records")
print(df_insurance.head(10))

"""## Pregunta
*¿Puedes identificar un atributo común entre los diferentes conjuntos de datos que permita juntarlos?*
"""

common_attributes = (
    set(df_retailbank.columns) & set(df_investment.columns) & set(df_insurance.columns)
)
print("Common attributes between datasets:", common_attributes)

# El atributo común entre los diferentes datasets es "ID" e identifica de manera única a cada cliente.
# Además, este puede usarse como clave para fusionar los datasets y consolidar la información de cada cliente proveniente de las distintas instituciones financieras.


"""## Pregunta
Indica cuál es la cantidad de registros en cada conjunto de datos.

*¿Qué conclusiones puedes sacar luego de observar los resultados?*
"""

print("RetailBankEFG - Number of records:", df_retailbank.shape[0])
print("InvestmentBankCDE - Number of records:", df_investment.shape[0])
print("InsuranceCompanyABC - Number of records:", df_insurance.shape[0])

# Todos los datasets tienen la misma cantidad de registros (10082), lo que sugiere que representan el mismo conjunto de clientes.
# Esto facilita la integración de los datos, ya que cada cliente tiene información disponible en las tres instituciones financieras.

"""## Pregunta
¿Has notado algún patrón entre los datos, ya sea entre filas o columnas?

# A partir de la exploración inicial de los datos, se pueden observar varios patrones interesantes:
# En primer lugar, muchas columnas de los datasets RetailBankEFG e InvestmentBankCDE representan
# productos financieros mediante valores binarios ('T' / 'F'), lo que indica si un cliente ha 
# contratado o no un determinado producto.

# Además, se observa que algunos clientes tienden a tener más de un producto financiero, lo que
# sugiere la existencia de posibles relaciones o correlaciones entre distintos tipos de financiamiento e inversión.

# En el dataset InsuranceCompanyABC, conviven variables númericas como la edad y la renta con variables categóricas
# como la región o los rangos de edad e ingresos. Esto sugiere que la información demográfica puede influir en la 
# contratación de productos fiancieros y seguros.

# En general, los datos presentan una estructura consistente, donde cada fila representa a un cliente
# y cada columna un atributo o producto específico, lo cual es adecuado para un análisis posterior y 
# para el desarrollo de modelos de machine learning.

# Evaluación de Calidad de Datos

## Valores Faltantes:
Vamos a identificar los valores nulos o faltantes en los conjuntos de datos. Para esto, crearás una función llamada `get_nan_values`. Esta función tomará como parámetro un dataframe y devolverá el número de valores nulos por fila y por columna.
"""


def get_nan_values(data_frame):
    # Count NaN values in each column
    nan_count_per_column = data_frame.isna().sum()
    # Total number of records with NaN values
    total_nan_records = data_frame.isna().any(axis=1).sum()
    # pass
    return {
        "Count NaN values in each column": nan_count_per_column,
        "Total number of records with NaN values": total_nan_records,
    }


"""*Imprime los valores faltantes por fila y columna*"""

# Write your code here for df_retailbank
print("NaN values in RetailBankEFG")
nan_retail = get_nan_values(df_retailbank)
print(nan_retail["Count NaN values in each column"])
print(
    "Total number of records with NaN values:",
    nan_retail["Total number of records with NaN values"],
)
# Write your code here for df_investment
print("NaN values in InvestmentBankCDE")
nan_investment = get_nan_values(df_investment)
print(nan_investment["Count NaN values in each column"])
print(
    "Total number of records with NaN values:",
    nan_investment["Total number of records with NaN values"],
)
# Write your code here for df_insurance
print("NaN values in InsuranceCompanyABC")
nan_insurance = get_nan_values(df_insurance)
print(nan_insurance["Count NaN values in each column"])
print(
    "Total number of records with NaN values:",
    nan_insurance["Total number of records with NaN values"],
)

"""## Pregunta
*¿Existen valores faltantes en los datos?*

No existen valores faltantes en los datos de los tres datasets, ya que el conteo de valores NaN por columna es cero y el total
de registros con valores NaN también es cero en cada uno de ellos.

## Duplicados
Vamos a detectar si existen filas duplicadas que pueden distorsionar los análisis. Para ello, vamos a validar si hay registros duplicados en el conjunto de datos utilizando la función `check_duplicates`. En caso afirmativo, necesitaremos pasar como parámetros el dataframe a validar y la columna que se utiliza como identificador.
"""


def check_duplicates(data_frame, column):
    """
    Check the number of duplicate values in the specified column(s) of a DataFrame.

    Parameters:
    data_frame (pandas.DataFrame): The DataFrame to check for duplicates.
    column (str or list): The column name or list of column names to check for duplicates.

    Returns:
    int: The number of duplicate rows.
    """
    # Check for duplicates
    duplicates_by_id = data_frame.duplicated(subset=column)

    # Count the number of duplicate rows
    num_duplicates = duplicates_by_id.sum()

    return num_duplicates


"""*Imprime la cantidad de filas duplicadas para df_retailbank, df_investment y df_insurance*"""

# Write your code here
print("Duplicate rows in RetailBankEFG (by ID):", check_duplicates(df_retailbank, "ID"))
print(
    "Duplicate rows in InvestmentBankCDE (by ID):",
    check_duplicates(df_investment, "ID"),
)
print(
    "Duplicate rows in InsuranceCompanyABC (by ID):",
    check_duplicates(df_insurance, "ID"),
)
"""## Pregunta
¿Existen datos duplicados?

Sí, existen datos duplicados en los tres datasets si se analiza la comuna ID como identificador.
Existen 552 IDs duplicados en cada uno de los conjuntos de datos, lo que indica que algunos clientes
aparecen más de una vez.

Esto no implica un error en los datos, ya que un cliente puede tener múltiples productos financieros o seguros
registrados. Sin embargo, es importante tener en cuenta estos duplicados al realizar análisis posteriores
o al desarrollar modelos de machine learning, ya que pueden influir en los resultados si no se manejan adecuadamente.

## Inconsistencias
En esta sección, se propondrán varios métodos para identificar inconsistencias en los datos. Primero, vamos a revisar las estadísticas básicas. Para ello, utilizaremos la función `describe()`.

*Imprime las estadísticas básicas*
"""

# Write your code here for df_retailbank
print("Basic statistics for RetailBankEFG:")
print(df_retailbank.describe(include="all").T)
# Write your code here for df_investment
print("Basic statistics for InvestmentBankCDE:")
print(df_investment.describe(include="all").T)
# Write your code here for df_insurance
print("Basic statistics for InsuranceCompanyABC:")
print(df_insurance.describe(include="all").T)

"""### Identificar Valores Únicos:
Ahora, para todas las variables no numéricas, debemos identificar cuántos tipos de datos están registrados en cada columna. Implementaremos la función `get_value_counts_non_numeric_columns`, la cual obtiene los conteos de valores de las columnas no numéricas en un DataFrame y devuelve un diccionario donde las claves son los nombres de las columnas no numéricas y los valores son sus respectivos conteos de valores.
"""


def find_non_numeric_columns(df):
    """
    Find non-numeric columns in a DataFrame.

    Parameters:
    df (pandas.DataFrame): The DataFrame to search for non-numeric columns.

    Returns:
    list: A list of non-numeric column names.
    """
    return df.select_dtypes(exclude=["number"]).columns.tolist()


"""*Implementa la función `get_value_counts_non_numeric_columns`, la cual debe hacer uso de la función `find_non_numeric_columns`. Esta función devuelve un diccionario donde las claves son los nombres de las columnas no numéricas y los valores son sus respectivos conteos de valores.*"""


def get_value_counts_non_numeric_columns(df):
    """
    Get the value counts of non-numeric columns in a DataFrame.

    Parameters:
    df (pandas.DataFrame): The DataFrame to analyze.

    Returns:
    dict: A dictionary where keys are non-numeric column names and values are their respective value counts.
    """
    # write your code here
    # Get non-numeric columns
    non_numeric_columns = find_non_numeric_columns(df)
    value_counts_dict = {}

    for col in non_numeric_columns:
        value_counts_dict[col] = df[col].value_counts()

    return value_counts_dict


"""*Imprime los conteos de las columnas no numéricas.*"""

# Write your code here for df_retailbank
print("Value counts for non-numeric columns in RetailBankEFG:")
retail_counts = get_value_counts_non_numeric_columns(df_retailbank)
for col, counts in retail_counts.items():
    print(f"Column: {col}")
    print(counts)

# Write your code here for df_investment
print("Value counts for non-numeric columns in InvestmentBankCDE:")
investment_counts = get_value_counts_non_numeric_columns(df_investment)
for col, counts in investment_counts.items():
    print(f"Column: {col}")
    print(counts)

# Write your code here for df_insurance
print("Value counts for non-numeric columns in InsuranceCompanyABC:")
insurance_counts = get_value_counts_non_numeric_columns(df_insurance)
for col, counts in insurance_counts.items():
    print(f"Column: {col}")
    print(counts)

"""### Verificar Tipos de Datos:
*Utiliza el atributo `dtypes` para verificar los tipos de datos de cada columna.*
"""

# Write your code here for df_retailbank
print("Data types for RetailBankEFG:")
print(df_retailbank.dtypes)
# Write your code here for df_investment
print("Data types for InvestmentBankCDE:")
print(df_investment.dtypes)
# Write your code here for df_insurance
print("Data types for InsuranceCompanyABC:")
print(df_insurance.dtypes)

"""## Pregunta
*¿Qué puedes concluir respecto de todas las variables que no son numéricas?*
Las variables no numéricas en los distintos datasets son de tipo categórico y se almacenan como objetos (object),
principalmente en forma de valores binarios (T/F) que indican la contratación o no de productos financieros o seguros.

*¿Has identificado algún patrón o característica?*
Sí, se observa que estas variables presentan un número reducido y bien definido de categorías, sin valores inconsistentes, lo que 
facilita su transformación a formato númerico mediante técnicas de codificación para su posterior uso en modelos de machine learning.

## Visualización General de los datos y Analizar Patrones Anómalos
Esta es una sección libre en la que podrás crear diferentes visualizaciones de los datos. Sugiero que utilices principalmente visualizaciones para validar la cantidad de datos de las variables no numéricas. Además, debes realizar gráficas tipo box plot para las columnas numéricas, exceptuando la columna ID.

## Por ejemplo:
### Visualizaciones para variables no numéricas:
- **Gráfico de barras:** Utiliza un gráfico de barras para visualizar la cantidad de datos únicos en cada variable no numérica.
- **Gráfico de pastel:** Muestra la distribución de los datos en cada variable no numérica utilizando un gráfico de pastel.

### Box plots para columnas numéricas:
- **Box plot para cada columna numérica (excluyendo la columna ID):** Utiliza box plots para visualizar la distribución de los datos, los valores atípicos y la mediana en cada columna numérica.
"""

# Write your code here, add your custom plots for df_retailbank
df_retailbank["Financiamento Casa"].value_counts().plot(
    kind="bar", title="Financiamiento Casa"
)
plt.show()
# Write your code here, add your custom plots for df_investment
df_investment["Investimento Poupanca"].value_counts().plot(
    kind="bar", title="Investimento Poupanca"
)
plt.show()
# Write your code here, add your custom plots for df_insurance
df_insurance["Regiao"].value_counts().plot(kind="bar", title="Regiao")
plt.show()

numeric_columns = ["Idade", "Renda"]
df_insurance[numeric_columns].boxplot()
plt.title("Box plots for numeric variables")
plt.show()

"""## Preguntas
1. *¿Cuál de las dos opciones sugieres utilizar para evaluar datos no numéricos: imprimir los valores o crear visualizaciones?*

Para evaluar datos no numéricos considero más adecuado crear visualizaciones que únicamente imprimir los valores,
ya que los gráficos permiten identificar de forma más rápida patrones, desbalances y distribuciones entre las categorías.

2. *¿Qué otros tipos de visualizaciones se te ocurren que podrías sugerir? Justifica tu respuesta.*

Otros tipos de visualizaciones que podrían utilizarse son gráficos de barras apiladas para comparar categorías entre distintos grupos,
o mapas de calor para analizar relaciones entre variables categóricas una vez codificadas.

3. *¿Existe un desbalance en los datos, es decir, existen más tipos que corresponden a una clase? ¿Cuál es la clase y cómo crees que esto puede afectar al construir modelos de machine learning?*

Sí, existe un desbalance en los datos, ya que en la mayoría de las variables categóricas predomina la clase "F" frente
a la positiva ("T"). Este desbalance puede afectar a los modelos de machine learning, haciendo que tiendan a predecir
la clase mayoritaria, por lo que será importante tenerlo en cuenta durante el entrenamiento y la evaluación del modelo.

### Analizar Patrones Anómalos:
Para realizar el análisis de patrones anómalos, utilizarás la función `plot_boxplot_violinplot`.

*Graficar la región(Regiao) en función de la edad(Idade), del conjunto de datos `df_insurance`.*
"""

# Write your code here for df_insurance
plot_boxplot_violinplot(x="Regiao", y="Idade", data_frame=df_insurance)
""" *Graficar la región(Regiao) en función de la edad(Renda), del conjunto de datos `df_insurance`.*"""

# Write your code here for df_insurance
plot_boxplot_violinplot(x="Regiao", y="Renda", data_frame=df_insurance)
"""## Preguntas
* *¿Cuál es la distribución de datos sugerida?*
* *¿Existen datos atípicos en el conjunto de datos?* *¿Cómo podrías corregir estos datos? Justifica tu respuesta*.

La distribución de los datos muestra que la variable edad tiene una distribución similar entre las distintas regiones, con
medianas cercanas y una dispersión moderada.

En el caso de la renta, la distribución es claramente asimétrica, con una alta concentración de valores bajos y la 
presencia de valores atípicos elevedaos en varias regiones.

Estos valores atípicos no necesariamente representan errores, pero pueden influir negativamente en el rendimiento de los 
modelos de machine learning. Para mitigar el impacto, podrían aplicarse técnica como la transformación logarítmica de 
la variable renta, la normalización de los datos o el tratamiento específico de outliers mediante recorte o escalado robusto.

# **Pregunta 2 - Limpieza y tratamiento de Datos**

# Limpieza de Datos

## Manejo de Valores Faltantes

### Preguntas
1. *¿Luego de la evaluación es necesario realizar alguna técnica para completar datos faltantes?*

No es necesario aplicar técnicas para completar valores faltantes, ya que no se han detectado  valores nulos en 
ninguno de los conjuntos de datos analizados.

2. *¿Debemos realizar tareas de imputación de valores luego de analizar los datos?*

No es necesario realizar tareas de imputación de valores, dado que los datos presentan una buena calidad y no
contienen valores faltantes que deban ser tratados antes del análisis o modelado posterior.

3. *¿Por favor, describe al menos dos técnicas de imputación de datos para valores faltantes basadas en métodos estadísticos?*

Imputación por la media/mediana: consiste en reemplazar los valores faltantes por la media o la mediana de la variable, siendo
la mediana más robusta frente valores atípicos.

Imputación por la moda: utilizada principalmente en variables categóricas, donde los valores faltantes se reemplazan por el valor
más frecuente.

4. *¿Por favor, describe al menos dos técnicas de imputación de datos para valores faltantes basadas en métodos predictivos?*

Imputación mediante modelos de regresión: se entrena un modelo predictivo para estimar los valores faltantes a partir de otras variables
del conjunto de datos.

Imputación utilizando algortimos de machine learning como k-NN, que estima los valores faltantes en función de la similitud con otros registros
completos.

## Eliminación de Duplicados

### Identificación y Eliminación:

*Vamos a eliminar los datos duplicados en todos los conjuntos de datos utilizando la función `drop_duplicates`, junto con el parámetro `inplace`.*
"""

# Write your code here
df_retailbank.drop_duplicates(subset="ID", inplace=True)
df_investment.drop_duplicates(subset="ID", inplace=True)
df_insurance.drop_duplicates(subset="ID", inplace=True)

print("RetailBankEFG shape after removing duplicates:", df_retailbank.shape)
print("InvestmentBankCDE shape after removing duplicates:", df_investment.shape)
print("InsuranceCompanyABC shape after removing duplicates:", df_insurance.shape)


"""## Pregunta

*¿Por qué es importante llevar a cabo la tarea de eliminación de duplicados? Por favor, justifica tu respuesta.*

Es importante eliminar duplicados para garantizar que cada cliente esté representado una sola vez en el conjunto de datos,
evitando que un mismo identificador (ID) aparezca repetido y genere sesgos en el análisis y en el entrenamiento de los
modelos de machine learning.

# Ingeniería de características

## Transformaciones
Las operaciones de ingeniería de características a menudo dependen de las relaciones entre las características, las cuales pueden distorsionarse al normalizar los datos. Luego, crear nuevas características como identificar los rangos de edades (Idade) y de ingresos (Renda) tiene más sentido en este punto. A continuación, se presenta un ejemplo al crear una nueva clase `CreateNewRangesColumns`, la cual implementa las clases y librerías necesarias para crear estas nuevas características.
"""


class CreateNewRangesColumns(BaseEstimator, TransformerMixin):

    def fit(self, X, y=None):
        # No adjustments needed in fit, simply return the object unchanged
        return self

    def createAgeRange(self, base_df):
        # Extract the age series from the base DataFrame
        age_series_temp = base_df["Idade"]

        # Define conditions to create age ranges
        conditions = [
            (age_series_temp >= 0) & (age_series_temp < 25),
            (age_series_temp >= 25) & (age_series_temp < 30),
            (age_series_temp >= 30) & (age_series_temp < 35),
            (age_series_temp >= 35) & (age_series_temp < 40),
            (age_series_temp >= 40) & (age_series_temp < 45),
            (age_series_temp >= 45) & (age_series_temp < 50),
            (age_series_temp >= 50) & (age_series_temp < 55),
            (age_series_temp >= 55) & (age_series_temp < 60),
            (age_series_temp >= 60),
        ]

        # Define choices for age ranges
        choices = [
            "R1-0-24",
            "R2-25-29",
            "R3-30-34",
            "R4-35-39",
            "R5-40-44",
            "R6-45-49",
            "R7-50-54",
            "R8-55-59",
            "R9-60",
        ]

        # Create 'AGE_RANGE' column based on defined conditions and choices
        base_df["AGE_RANGE"] = np.select(conditions, choices, default="UNKNOWN")

        return base_df

    def createIncomeRange(self, base_df):
        # Convert income series to numeric format
        income_series_temp = pd.to_numeric(base_df["Renda"], errors="coerce")

        # Define conditions to create income ranges
        conditions = [
            (income_series_temp <= 6000),
            (income_series_temp >= 6000) & (income_series_temp < 6500),
            (income_series_temp >= 6500) & (income_series_temp < 7000),
            (income_series_temp >= 7000) & (income_series_temp < 7500),
            (income_series_temp >= 7500) & (income_series_temp < 8000),
            (income_series_temp >= 8000) & (income_series_temp < 8500),
            (income_series_temp >= 8500) & (income_series_temp < 9000),
            (income_series_temp >= 9000),
        ]

        # Define choices for income ranges
        choices = [
            "R1-6000",
            "R2-6000-6500",
            "R3-6500-7000",
            "R4-7000-7500",
            "R5-7500-8000",
            "R6-8000-8500",
            "R7-8500-9000",
            "R8-9000",
        ]

        # Create 'INCOME_RANGE' column based on defined conditions and choices
        base_df["INCOME_RANGE"] = np.select(conditions, choices, default="UNKNOWN")

        return base_df

    def transform(self, X):
        # First, make a copy of the input DataFrame 'X'
        data = X.copy()

        # Create the age range column
        df_with_age_range = self.createAgeRange(data)

        # Create the income range column
        df_with_income_range = self.createIncomeRange(df_with_age_range)

        return df_with_income_range


"""A continuación, te presentamos un ejemplo de cómo utilizar esta clase(`CreateNewRangesColumns`). Después, podrás observar que el DataFrame `df_insurance` ahora cuenta con dos nuevas columnas: `AGE_RANGE` e `INCOME_RANGE`, las cuales contienen la información de la identificación de nuevos grupos de datos.

"""

df_insurance = CreateNewRangesColumns().fit_transform(df_insurance)
df_insurance.head(10)

"""Imprimimos las nuevas columnas"""

df_insurance.info()

"""Ahora visualizamos las nuevas escalas de los datos mediante un gráfico de barras.

"""

plot_count_plots(df_insurance, ["AGE_RANGE", "INCOME_RANGE"])

"""## Corrección de Inconsistencias

### Estándar de Formato:
A continuación vamos a normalizar los datos numéricos. Luego convertiremos las variables categóricas de Falso (F) y Verdadero (T) a valores numéricos binarios: 0 para Falso y 1 para Verdadero.

### Reemplazar Valores Erróneos:
Como pudiste observar en las gráficas anteriores, existen diferentes valores atípicos que se encuentran fuera del rango. Para abordar esto, vamos a crear una opción que nos permita excluir los datos atípicos de nuestro conjunto de datos `df_insurance`. Para tomar esta decisión, eliminaremos todos los registros que sean menores al primer cuartil y todos aquellos mayores al tercer cuartil. El resultado final se asignará al DataFrame `df_insurance`. Para llevar a cabo este proceso, haremos uso de la clase `OutlierRemover`.
"""


# Custom transformer to remove outliers from specified columns
class OutlierRemover(BaseEstimator, TransformerMixin):
    def __init__(self, threshold=1.5, columns=None):
        # Initialize with a threshold and list of columns to check for outliers
        self.threshold = threshold
        self.columns = columns

    def fit(self, X, y=None):
        # No fitting necessary for outlier detection based on IQR
        return self

    def transform(self, X, y=None):
        X = X.copy()
        # Convert to DataFrame if necessary for easier manipulation
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X)

        # If no specific columns are provided, use all columns
        if self.columns is None:
            self.columns = X.columns

        # Calculate the 1st (Q1) and 3rd (Q3) quartiles for specified columns
        Q1 = X[self.columns].quantile(0.25)
        Q3 = X[self.columns].quantile(0.75)
        # Calculate the interquartile range (IQR)
        IQR = Q3 - Q1

        # Define the lower and upper bounds for detecting outliers
        lower_bound = Q1 - self.threshold * IQR
        upper_bound = Q3 + self.threshold * IQR

        # Create a mask to identify rows with any feature values outside the bounds
        mask = (
            (X[self.columns] >= lower_bound) & (X[self.columns] <= upper_bound)
        ).all(axis=1)

        # Keep only the rows that are within the bounds
        X_filtered = X[mask].reset_index(drop=True)

        # Replace the original specified columns with the filtered values
        X[self.columns] = X_filtered[self.columns]

        return X.dropna()


"""*Ejecuta la transformación utilizando la clase `OutlierRemover` y asigna el resultado a `df_insurance`*"""

# Write your code here

outlier_remover = OutlierRemover(threshold=1.5, columns=["Idade", "Renda"])

df_insurance = outlier_remover.fit_transform(df_insurance)

"""## Pregunta
Después de eliminar los datos atípicos, ¿cuántos registros tiene ahora el DataFrame `df_insurance`?
"""

# Write your code here

print("Number of records after removing outliers:", df_insurance.shape[0])

"""## Pregunta
*Explica con tus propias palabras cómo podría afectar una diferencia significativa en el tamaño del conjunto de datos antes y después de eliminar los valores atípicos. ¿Qué implicaciones podría tener esto en los resultados de un modelo de machine learning?*

Tras eliminar los datos atípicos, el conjunto de datos df_insurance ha pasado de tener 9530 registros a 8844, lo que supone una reducción significativa
del tamaño del conjunto de datos.

La reducción podría tener impacto en los modelos de machine learning, ya que si se eliminan demasiados registros, el modelo podría entrenarse con
menos información, lo que podría afectar su capacidad para generalizar y predecir correctamente en datos nuevos.

Aunque eliminar valores atípicos extremos también puede mejorar el rendimiento del modelo evitando que dichos valores distorsionen el proceso de entrenamiento.

Por lo que, es crucial encontrar un equilibrio adecuado entre eliminar datos atípicos y mantener suficiente información para entrenar modelos robustos.

## Gráficos luego de eliminar datos atípicos

En las siguientes gráficas, puedes observar las diferencias con respecto a las del apartado inicial "Analizar Patrones Anómalos".

*Graficar la región(Regiao) en función de la edad(Idade), del conjunto de datos `df_insurance`, utilizando la función `plot_boxplot_violinplot`.*
"""

# Write your code here

plot_boxplot_violinplot(x="Regiao", y="Idade", data_frame=df_insurance)

""" *Graficar la región(Regiao) en función de los ingresos(Renda), del conjunto de datos `df_insurance`, utilizando la función `plot_boxplot_violinplot`.*"""

# Write your code here, add you plot using ´plot_boxplot_violinplot´

plot_boxplot_violinplot(x="Regiao", y="Renda", data_frame=df_insurance)

"""## Pregunta
*¿Cómo crees que la eliminación de datos atípicos ha afectado la distribución y los patrones observados en las gráficas? ¿Qué cambios específicos puedes identificar en los datos después de esta eliminación?*

La eliminacion de los datos atípicos ha reducido la dispersión de las distribuciones, especialmente en la variable de ingresos, donde se
observaban valores extremos.

Tras esta limpieza, los boxplots muestran rangos más acotados y una distribución más homogénea entre regiones, lo que facilita la
interpretación de los patrones y reduce el impacto de valores extremos en el entrenamiento de modelos de machine learning.

## Normalización y Escalado
### Estandarización:
Vamos a convertir los datos numéricos a una escala común. Para esto, vamos a aplicar la estandarización sobre las columnas numéricas "Idade" y "Renda". Luego, para ajustar la escala, vamos a utilizar las clases StandardScaler y ColumnTransformer. Debes implementar el código necesario para realizar la conversión.

Para la clase DataScaleImputer, debes investigar un poco cómo realizar la conversión a una escala estándar (StandardScaler) mediante el uso de la clase ColumnTransformer. Puedes revisar la documentación oficial de sklearn.

Otra opción es crear una función que permita estandarizar las características eliminando la media y escalando a varianza unitaria. Esto se calcula como: z = (x - u) / s
"""


# Custom class to impute and scale data
class DataScaleImputer(BaseEstimator, TransformerMixin):
    def __init__(self, columns):
        self.columns = columns  # Columns to be scaled
        self.scaler = StandardScaler()  # Initialize the StandardScaler

    def fit(self, X, y=None):
        # Fit the scaler on the specified columns
        self.scaler.fit(X[self.columns])
        return self  # Return the transformer

    def transform(self, X):
        data = (
            X.copy()
        )  # Make a copy of the input DataFrame to avoid modifying the original

        # Create a ColumnTransformer that will apply StandardScaler only to the specified columns
        # Write you code here, change None by custom transformer
        transformer = ColumnTransformer(
            transformers=[("scaler", self.scaler, self.columns)], remainder="drop"
        )

        # Apply the transformer to the data
        X_transform = transformer.fit_transform(data)

        # Convert the result to a DataFrame to maintain the column labels
        X_imputed_df = pd.DataFrame(data=X_transform, columns=self.columns)

        # Replace the original columns in 'data' with the scaled columns
        data[self.columns] = X_imputed_df[self.columns]

        return data.dropna()  # Return the transformed DataFrame


"""*Ejecuta la transformación utilizando la clase `DataScaleImputer` y asigna el resultado a `df_insurance`*"""

# Write you code here
scaler = DataScaleImputer(columns=["Idade", "Renda"])
df_insurance = scaler.fit_transform(df_insurance)


"""*Imprime las estadísticas básicas del conjunto de datos df_insurance, ubásicas utilizando el método `describe()`*"""

# Write you code here
print(df_insurance[["Idade", "Renda"]].describe())

"""## Pregunta
*¿Cuáles otras técnicas conoces que pueden ser utilizadas para escalar o normalizar los datos? Menciona dos.*

Otras técnicas para escalar o normalizar los datos son el Min-Max Scaling, que reescala los valores a un rango fijo (por ejemplo, entre 0 y 1),
y el Robust Scaling, que utiliza la mediana y el rango intercuartílico y es menos sensible a valores atípicos.

## Unificación de conjuntos de datos

Vamos a unificar diferentes conjuntos de datos (`df_insurance`, `df_retailbank` y `df_investment`) para crear un nuevo DataFrame. Utilizaremos la función `merge` de Pandas, identificando previamente el atributo que nos permitirá integrar estos conjuntos como uno solo. El resultado final se asignará a la variable `data_frame_merged`. A continuación, mostraremos los primeros 10 registros.

*Utiliza la función `merge` de Pandas para fusionar los conjuntos de datos en uno solo, asignándolo a la variable `data_frame_merged`.*
"""

# Write you code here

data_frame_merged = df_insurance.merge(df_retailbank, on="ID", how="inner").merge(
    df_investment, on="ID", how="inner"
)

print(data_frame_merged.head(10))

"""*Imprime la cantidad total de registros después de realizar el merge entre los conjuntos de datos.*"""

# Write you code here

print("Total number of records after merge:", data_frame_merged.shape[0])

"""*Observamos una visión estadística rápida de los datos mediante la función `describe`.*"""

# Write you code here

print(data_frame_merged.describe())

"""*Verifica si hay datos faltantes en el DataFrame resultante.*"""

# Write you code here
print("Missing values per column after merge:")
print(data_frame_merged.isna().sum())

"""# Correcion nombres columnas

Como has notado, se presentan ciertos inconvenientes en los nombres de las columnas. A continuación, intentaremos resolver estos errores identificando y corrigiendo espacios adicionales u otros problemas.
"""

data_frame_merged = data_frame_merged.rename(
    columns=lambda x: re.sub("[^A-Za-z0-9_]+", "", x)
)
data_frame_merged.info()

"""## Finalización tramiento de datos

### Tratamiento de datos para modelos de Machine Learning

Como último paso, es necesario ejecutar el siguiente tratamiento a los datos con el objetivo de prepararlos para nuestros modelos de Machine Learning. Seguiremos los siguientes pasos:

1. **Creación de la variable a predecir:** Se creará una nueva columna llamada "tipo_financiamiento", que será la variable a predecir por nuestros modelos de Machine Learning. Esta columna representará el tipo de financiamiento, permitiendo identificar si corresponde a una casa, un carro, ambos o ninguno. El siguiente proceso se ejecutará sobre el `data_frame_merged`, generando un nuevo DataFrame llamado `data_frame_tipo_financiamiento` con la columna adicional "tipo_financiamiento".

2. **Etiquetado de columnas categóricas:** Se etiquetarán las columnas categóricas como multi label. Las columnas identificadas son "AGE_RANGE", "INCOME_RANGE", "tipo_financiamiento" y "Regiao".

3. **Eliminación de la columna "ID":** Se eliminará la columna utilizada como identificador.

4. **Conversión de valores binarios:** Se convertirán todas las columnas con valores 'F' o 'T' a tipos de datos numéricos 0 y 1, respectivamente.
"""


class OneHotDecoderImputer(BaseEstimator, TransformerMixin):
    def __init__(self, columns, label_column_name):
        """
        Initialize the OneHotDecoderImputer.

        Parameters:
        - columns: list of str, names of columns to be converted from one-hot encoding
        - label_column_name: str, name of the new label column
        """
        self.columns = (
            columns  # List of column names to be converted from one-hot encoding
        )
        self.label_column_name = label_column_name
        self.label_encoders = {}  # Dictionary to store label encoders for each column

    def fit(self, X, y=None):
        """
        Fit the label encoders on the specified columns.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - self: OneHotDecoderImputer, the transformer instance
        """
        for col in self.columns:
            encoder = LabelEncoder()
            encoder.fit(X[col])
            self.label_encoders[col] = encoder
        return self  # Return the transformer instance

    def get_financing_type_name_from_row(self, row):
        """
        Get the financing type name from a one-hot encoded row.

        Parameters:
        - row: pd.Series, a row of one-hot encoded data

        Returns:
        - str or None, the name of the financing type or None if not found
        """
        total_financing_types = row.sum()
        if total_financing_types == len(row):
            return "Ambos"
        if total_financing_types == 0:
            return "Ninguno"
        for col_name, value in row.items():
            if value == 1:
                return col_name
        return None

    def transform(self, X):
        """
        Convert one-hot encoded columns to a single label column.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - X_transformed: pd.DataFrame, the DataFrame with the new label column
        """
        # Create a copy of the input DataFrame to keep the original data
        X_transformed = X.copy()

        # Encode the specified columns using the fitted label encoders
        for col in self.columns:
            X_transformed[col] = self.label_encoders[col].transform(X[col])

        # Create the label column by applying the method to each row
        X_transformed[self.label_column_name] = X_transformed[self.columns].apply(
            lambda row: self.get_financing_type_name_from_row(row), axis=1
        )

        return X_transformed.drop(columns=self.columns)


class BooleanToNumeric(BaseEstimator, TransformerMixin):
    def __init__(self):
        """
        Initialize the BooleanToNumeric transformer.
        """
        pass

    def fit(self, X, y=None):
        """
        Fit the BooleanToNumeric transformer.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - self: BooleanToNumeric, the transformer instance
        """
        return self

    def transform(self, X):
        """
        Transform boolean values ("T" or "F") to numerical values (1 or 0).

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - X_transformed: pd.DataFrame, the transformed DataFrame
        """
        X_transformed = X.replace({"T": 1, "F": 0})
        return X_transformed


class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, columns=None):
        """
        Initialize the MultiColumnLabelEncoder.

        Parameters:
        - columns: array of str, names of columns to encode. If None, encode all columns.
        """
        self.columns = columns
        self.label_encoders = {}

    def fit(self, X, y=None):
        """
        Fit the label encoders on the specified columns.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - self: MultiColumnLabelEncoder, the transformer instance
        """
        if self.columns is None:
            self.columns = X.columns
        for col in self.columns:
            self.label_encoders[col] = LabelEncoder().fit(X[col])
        return self

    def transform(self, X):
        """
        Transform the specified columns using the fitted label encoders.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - X_transformed: pd.DataFrame, the DataFrame with transformed columns
        """
        X_transformed = X.copy()
        for col in self.columns:
            X_transformed[col] = self.label_encoders[col].transform(X[col])
        return X_transformed

    def fit_transform(self, X, y=None):
        """
        Fit label encoders on the specified columns and transform the DataFrame.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - X_transformed: pd.DataFrame, the DataFrame with transformed columns
        """
        return self.fit(X, y).transform(X)

    def inverse_transform(self, X):
        """
        Reverse the encoding back to the original values.

        Parameters:
        - X: pd.DataFrame, the DataFrame with encoded columns

        Returns:
        - X_inverse: pd.DataFrame, the DataFrame with original values
        """
        X_inverse = X.copy()
        for col in self.columns:
            X_inverse[col] = self.label_encoders[col].inverse_transform(X[col])
        return X_inverse


class DropColumns(BaseEstimator, TransformerMixin):
    def __init__(self, columns=None):
        """
        Initialize the DropColumns transformer.

        Parameters:
        - columns: list of str, names of columns to drop from the DataFrame
        """
        self.columns = columns

    def fit(self, X, y=None):
        """
        Fit the DropColumns transformer.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - self: DropColumns, the transformer instance
        """
        return self

    def transform(self, X):
        """
        Transform the input DataFrame by dropping specified columns.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - X_transformed: pd.DataFrame, the transformed DataFrame
        """
        X_transformed = X.drop(columns=self.columns, errors="ignore")
        return X_transformed


"""Definimos el pipeline para ajustar los datos al formato requerido para resolver el ejercicio."""

pipeline_data_preparation = Pipeline(
    [
        (
            "one_hote_to_label",
            OneHotDecoderImputer(
                columns=["FinanciamentoCasa", "FinanciamentoCarro"],
                label_column_name="tipo_financiamiento",
            ),
        ),
        (
            "label_encode",
            MultiColumnLabelEncoder(
                columns=["AGE_RANGE", "INCOME_RANGE", "tipo_financiamiento", "Regiao"]
            ),
        ),
        ("drop_columns", DropColumns(columns=["ID"])),
        ("boolean_numeric", BooleanToNumeric()),
    ]
)

"""*Ejecuta el pipeline para ajustar los datos y asignarlos a la variable `data_frame_tipo_financiamiento`.*"""

data_frame_tipo_financiamiento = pipeline_data_preparation.fit_transform(
    data_frame_merged
)
data_frame_tipo_financiamiento.head(10)

"""Obtenemos las etiquetas por tipo de financiamiento y asignamos a la varabile `le_tipo_financiamiento_mapping`."""

le = pipeline_data_preparation[1].label_encoders["tipo_financiamiento"]
le_tipo_financiamiento_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
tipo_financiamiento_mapping = {v: k for k, v in le_tipo_financiamiento_mapping.items()}
tipo_financiamiento_mapping

"""*Imprime las estadísticas básicas del conjunto de datos `data_frame_tipo_financiamiento`*"""

# Write you code here
print(data_frame_tipo_financiamiento.describe())

"""## Cierre tratamiento de datos
Es crucial comprender que el tratamiento de datos no es solo una etapa preliminar, sino un proceso continuo que puede influir significativamente en el rendimiento y la precisión de los modelos de Machine Learning. Al abordar de manera efectiva problemas como valores faltantes, valores atípicos y errores de formato, estamos creando un conjunto de datos robusto y confiable, lo que a su vez potencia la capacidad predictiva de nuestros modelos.

Hasta este punto, hemos completado varios pasos relacionados con el tratamiento y la limpieza de datos. Ahora vamos a continuar con el desarrollo de los diferentes algoritmos de Machine Learning.

*Exporta el DataFrame data_frame_tipo_financiamiento a un archivo CSV sin incluir el índice*
"""

# Write you code here

data_frame_tipo_financiamiento.to_csv("data_frame_tipo_financiamiento.csv", index=False)

"""# **Pregunta 3 - Creación de modelos de Machine Learning**

# Selección de modelo:
Identifica el tipo de problema de aprendizaje (clasificación, regresión, agrupamiento, etc.) y selecciona los modelos más adecuados para tu problema.
Experimenta con diferentes algoritmos de machine learning y ajusta sus hiperparámetros para encontrar la mejor combinación.
1. Entrenamiento de modelos:
Entrena por lo menos 3 modelos utilizando los datos de entrenamiento. Ajusta los parámetros del modelo utilizando algoritmos de optimización como la descenso del gradiente, búsqueda de cuadrícula, o búsqueda aleatoria.
Valida el modelo utilizando los datos de validación y ajusta los parámetros según sea necesario para evitar el sobreajuste (overfitting).
2. Evaluación del modelo:
Evalúa el rendimiento del modelo utilizando métricas apropiadas para tu problema (precisión, recall, F1-score, matriz de confusión, etc.).
Utiliza técnicas de validación cruzada para obtener estimaciones más robustas del rendimiento del modelo.

## Pregunta
*¿Cuál es el tipo de problema que estás enfrentando: clasificación o regresión? Imprime o grafica el conteo de valores que corresponde a la columna `data_frame_tipo_financiamiento`.*
"""

# Write you code here
print("Value counts for tipo_financiamiento:")
print(data_frame_tipo_financiamiento["tipo_financiamiento"].value_counts())

# Write you code here, add your custom plot
data_frame_tipo_financiamiento["tipo_financiamiento"].value_counts().plot(
    kind = "bar",
    title = "Distribución de la variable objetivo: tipo_financiamineto"
)
plt.xlabel("Tipo de financiamiento")
plt.ylabel("Número de registros")
plt.show()

# El problema corresponde a uno de clasificación multiclase, ya que la variable objetivo 'tipo_financiamiento' 
# toma valores discretos que representan diferentes categorías.
# Al observar el conteo de clases, se identifica un desbalance en los datos, siendo la clase asociada a "Ambos" 
# la más frecuente, mientras que otras categorías presentan menor número de registros.

"""## Pasos para el entrenamiento de modelos

A continuación, desarrolla los siguientes pasos para cada uno de los modelos sobre el conjunto de datos `data_frame_tipo_financiamiento`:
* **División del conjunto de datos:** Divide los datos en conjuntos de entrenamiento y prueba. El conjunto de entrenamiento se utiliza para entrenar el modelo, mientras que el conjunto de prueba se utiliza para evaluar su rendimiento.

* **Selección de modelo:** Elige el algoritmo de aprendizaje automático más adecuado para tu problema. Esto depende del tipo de problema (regresión, clasificación, clustering, etc.), el tamaño y la naturaleza de los datos, y los requisitos de rendimiento.

* **Entrenamiento del modelo:** Utiliza el conjunto de entrenamiento para ajustar los parámetros del modelo. Durante este proceso, el modelo aprenderá a mapear las características de entrada a las etiquetas de salida.

* **Validación del modelo:** Evalúa el rendimiento del modelo utilizando el conjunto de prueba. Esto te permite verificar si el modelo generaliza bien a datos no vistos y detectar posibles problemas de sobreajuste o subajuste.

### **Pasos para el entrenamiento del modelo - (Nombre Modelo)**
"""

# Load your dataset
# Assuming your data is stored in a DataFrame called 'data_frame_tipo_financiamiento'
# and the target variable is in a column called 'tipo_financiamiento'
# Replace 'data_frame_tipo_financiamiento' and 'tipo_financiamiento' with your actual DataFrame and column names
# Write you code here

# Split the data into training and testing sets using startified_train_test_split
# You can adjust the test_size parameter as needed
# 'random_state' ensures reproducibility of results
# Write you code here

# Create the custom model
# You can customize the parameters based on your requirements
# Write you code here

# Train the model on the training data
# Write you code here

# Make predictions on the testing data
# Write you code here

"""### **Evaluación del modelo - (Nombre Modelo)**"""

# Evaluate accuracy the model
# Write you code here

# Plot accuracy the model over the time - use plot_accuracy_scores
# plot_accuracy_scores(rf_model,X_train,y_train,X_test,y_test,nparts=5,jobs=2)

# Print classifitacion report using classification_report

# Plot confusion matrix using plot_confusion_matrix

"""### **Pasos para el entrenamiento del modelo  a comparar - (LogisticRegression)**"""

# Load your dataset
# Assuming your data is stored in a DataFrame called 'data_frame_tipo_financiamiento'
# and the target variable is in a column called 'tipo_financiamiento'
# Replace 'data_frame_tipo_financiamiento' and 'tipo_financiamiento' with your actual DataFrame and column names
X = data_frame_tipo_financiamiento.drop(columns=["tipo_financiamiento"])  # Features
y = data_frame_tipo_financiamiento["tipo_financiamiento"]  # Target variable

# Split the data into training and testing sets
# You can adjust the test_size parameter as needed
# 'random_state' ensures reproducibility of results
X_train, X_test, y_train, y_test = startified_train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create the Random LogisticRegression
# You can customize the parameters based on your requirements
lr_model = LogisticRegression(
    C=1.0,
    class_weight=None,
    dual=False,
    fit_intercept=True,
    intercept_scaling=1,
    l1_ratio=None,
    max_iter=1000,
    multi_class="multinomial",
    n_jobs=None,
    penalty="l2",
    random_state=1355,
    solver="lbfgs",
    tol=0.0001,
    verbose=0,
    warm_start=False,
)

# Train the model on the training data
lr_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = lr_model.predict(X_test)

"""### **Evaluación del modelo - (LogisticRegression)**"""

# Evaluate accuracy the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Plot accuracy the model over the time
plot_accuracy_scores(lr_model, X_train, y_train, X_test, y_test, nparts=5, jobs=2)

# Print classifitacion report
clas_report = classification_report(y_test, y_pred, labels=np.unique(y_pred), digits=6)
print(clas_report)

# Plot confusion matrix
plot_confusion_matrix(confusion_matrix(y_test, y_pred), tipo_financiamiento_mapping)

"""## Preguntas
* *¿Puedes comparar los modelos y determinar cuál de ellos tiene un mejor rendimiento en términos de exactitud?*
* *¿Logran los modelos etiquetar todas las clases de forma precisa? ¿Qué estrategias podrían aplicarse para mejorar este aspecto?*

## Extracción de características y Análisis de Componentes Principales(PCA)

Aquí está la corrección:

Ahora vamos a desarrollar validaciones para ver cuáles características son más relevantes para el modelo. Para esto, debes a implementar una función llamada  `plot_correlations` que te permita graficar las correlaciones del DataFrame `data_frame_tipo_financiamiento`.
"""


def plot_correlations(df_temp):
    # Write your code here
    pass


# Write your code here, plot using plot_correlations
plot_correlations(data_frame_tipo_financiamiento)

"""## Pregunta
* *¿Puedes identificar cuáles columnas son más relevantes y por qué?*

La siguiente función, `get_most_important_features`, nos permite extraer aquellas n columnas más relevantes a partir de la matriz de correlación.
"""


def get_most_important_features(correlation_matrix, target_column, n=5):
    """
    Get the top N most important features based on their absolute correlation values.

    Parameters:
    - correlation_matrix: pd.DataFrame, the correlation matrix
    - target_column: str, the name of the target variable column
    - n: int, the number of top features to return

    Returns:
    - top_features: list, the top N most important feature names
    """
    # Get the absolute correlation values with the target variable
    correlation_with_target = (
        correlation_matrix[target_column].abs().sort_values(ascending=False)
    )

    # Exclude the target variable itself
    correlation_with_target = correlation_with_target.drop(target_column)

    # Get the top N most important features
    top_features = correlation_with_target.head(n).index.tolist()

    return top_features


"""*Utiliza la función `get_most_important_features` e imprime las primeras 6 columnas más relevantes del conjunto de datos que están relacionadas con la variable a predecir.*

"""

# Write your code here

"""# Análisis de Componentes Principales(PCA)

## Pregunta
*¿Qué es el análisis de componentes principales y cuál es su utilidad al implementar modelos de machine learning?*

*Modifica la función `create_pca_model`. Los parámetros de entrada son el conjunto de datos sin la variable a predecir. Se creará un modelo de Análisis de Componentes Principales (PCA), el cual tendrá como parámetro el número N de componentes a identificar. El resultado será el modelo exportado y la transformación hacia las componentes principales luego de evaluar el modelo.*

* Esta función toma como entrada `X_train`, que representa las características del conjunto de entrenamiento sin la variable objetivo, y `n_components`, que indica el número de componentes principales que se desea conservar.

* Dentro de la función, se instancia un objeto PCA con el número de componentes especificado. Luego, se ajusta este objeto PCA a las características del conjunto de entrenamiento para determinar las características transformadas.

* Estas características transformadas se almacenan en un DataFrame llamado `X_principal`, que luego se devuelve junto con el objeto PCA ajustado (`pca_model`) como salida de la función.
"""


def create_pca_model(X_train, n_components):
    """
    Create a Principal Component Analysis (PCA) model.

    Parameters:
    X_train (DataFrame): The training dataset without the target variable.
    n_components (int): The number of principal components to identify.

    Returns:
    pca_model (PCA): The fitted PCA model.
    X_principal (DataFrame): The transformed features into principal components.
    """
    # Instantiate PCA
    # Write your code here
    pca_model = None

    # Fit PCA to the training data and transform features
    # Write your code here
    X_principal = None

    # Return pca_model,X_principal
    return pca_model, X_principal


"""Llamamos a la función `create_pca_model`, pasando como argumentos el DataFrame `data_frame_tipo_financiamiento` y `n_components` igual a 10, para determinar las 10 componentes principales del conjunto de datos. Luego, graficamos la varianza acumulada."""

pca_model, X_principal = create_pca_model(
    data_frame_tipo_financiamiento.drop(columns=["tipo_financiamiento"]),
    n_components=10,
)
plot_pca_cumulative_variance(pca_model)

"""A continuación, obtenemos la lista de los N componentes principales."""

df_pca_components = get_pca_components(
    pca_model,
    data_frame_tipo_financiamiento.drop(columns=["tipo_financiamiento"]).columns,
)
df_pca_components

"""## Pregunta
*Compara las variables obtenidas después de realizar el PCA en el conjunto de datos con las variables identificadas a través de la matriz de confusión. ¿Has encontrado coincidencias entre las variables y qué conclusiones puedes extraer de esto?*

Vamos a graficar la curva conocida como codo (elbow curve) utilizando la función `plot_elbow_curve_pca`.
"""

plot_elbow_curve_pca(X_principal)

"""## Pregunta
*Primero, investiga para qué sirve la curva conocida como codo (elbow curve). Luego, responde a la pregunta: ¿Cuántos componentes principales (columnas) puedes sugerir que sean utilizados por algún modelo de Machine Learning?*

*Establece el valor para la variable `n_components_pca`, luego ejecuta el modelo de aprendizaje, que incluye una tarea de reducción de la dimensionalidad mediante PCA (Análisis de Componentes Principales).*
"""

n_componenets_pca = 2

X = data_frame_tipo_financiamiento.drop(columns=["tipo_financiamiento"])
y = data_frame_tipo_financiamiento["tipo_financiamiento"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Define the steps of the pipeline
steps = [
    (
        "pca",
        PCA(n_components=n_componenets_pca),
    ),  # Apply PCA to reduce dimensionality to 2 components
    (
        "clf",
        GradientBoostingClassifier(
            ccp_alpha=0.0,
            criterion="friedman_mse",
            init=None,
            learning_rate=0.1,
            loss="log_loss",
            max_depth=3,
            max_features=None,
            max_leaf_nodes=None,
            min_impurity_decrease=0.0,
            min_samples_leaf=1,
            min_samples_split=2,
            min_weight_fraction_leaf=0.0,
            n_estimators=100,
            n_iter_no_change=None,
            random_state=447,
            subsample=1.0,
            tol=0.0001,
            validation_fraction=0.1,
            verbose=0,
            warm_start=False,
        ),
    ),  # Example classifier
]

# Create the pipeline
pipeline = Pipeline(steps)

# Train the model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)

# Print classifitacion report
clas_report = classification_report(y_test, y_pred, labels=np.unique(y_pred), digits=6)
print(clas_report)

# Plot confusion matrix
plot_confusion_matrix(confusion_matrix(y_test, y_pred), tipo_financiamiento_mapping)

"""Interaction Terms:

Create new features that capture the interactions between existing features. For instance, combining pairs of binary features using logical operations like AND, OR might uncover useful patterns.

## Implementación del tratamiento de datos desbalanceados
"""

X = data_frame_tipo_financiamiento.drop(columns=["tipo_financiamiento"])
y = data_frame_tipo_financiamiento["tipo_financiamiento"]

"""A continuación, se muestra una gráfica que ilustra la presencia de datos desbalanceados.



"""

conteo_tipo_financiamiento_label = y.value_counts().rename(
    index=tipo_financiamiento_mapping
)
conteo_tipo_financiamiento_label.plot.pie()
y.value_counts()

"""Para abordar el problema, vamos a comenzar reduciendo la variable que tiene mayor presencia y luego crearemos nuevos datos sintéticos para que los datos con menor presencia tengan la misma representatividad."""

# Define the steps of the pipeline
# Calculating class counts
class_counts = data_frame_tipo_financiamiento["tipo_financiamiento"].value_counts()

# Setting sampling strategy
sampling_strategy = {3: int(class_counts.max() * 0.30), 1: int(class_counts[1] * 0.20)}

# Undersampling with RandomUnderSampler
rand_under = RandomUnderSampler(sampling_strategy=sampling_strategy)

# Oversampling with SMOTE
smote_over = SMOTE(sampling_strategy="not majority", k_neighbors=5, random_state=42)

# Steps for addressing imbalance
steps_imbalance = [("sampling_under", rand_under), ("sampling", smote_over)]

# Create the pipeline
pipeline_fix_imbalance = Pipeline(steps_imbalance)
pipeline_fix_imbalance.fit(X, y)

# Resample the data
X_reshaped, y_reshaped = pipeline_fix_imbalance.fit_resample(X, y)

"""*Implementa un gráfico tipo pie que muestre cómo lucen los datos después de realizar el tratamiento para abordar el desbalance.*"""

# Write your code here
# conteo_tipo_financiamiento_label = y_reshaped.value_counts().rename(index=tipo_financiamiento_mapping)
# conteo_tipo_financiamiento_label.plot.pie()
# y_reshaped.value_counts()

"""*Separa los datos en conjuntos de entrenamiento y test utilizando la función `startified_train_test_split()`. Luego, implementa un modelo que haga uso del siguiente clasificador. Puedes probar modificando los hiperparámetros y evaluar los resultados. También puedes optar por modificar los parámetros de las clases `RandomUnderSampler` y `SMOTE` del paso anterior.*


```
GradientBoostingClassifier(
        ccp_alpha=0.0,
        criterion='friedman_mse',
        init=None,
        learning_rate=0.1,
        loss='log_loss',
        max_depth=3,
        max_features=None,
        max_leaf_nodes=None,
        min_impurity_decrease=0.0,
        min_samples_leaf=1,
        min_samples_split=2,
        min_weight_fraction_leaf=0.0,
        n_estimators=100,
        n_iter_no_change=None,
        random_state=8860,
        subsample=1.0,
        tol=0.0001,
        validation_fraction=0.1,
        verbose=0,
        warm_start=False)
```


"""

# Split the data into training and testing sets with stratification
# Stratification ensures that the class distribution is preserved in both training and testing sets
# Write your code here


# Define the steps for the pipeline
steps_gradient_boost = [
    ("sampling_under", rand_under),  # Undersampling
    ("sampling_over", smote_over),  # Oversampling
    (
        "clf",
        GradientBoostingClassifier(
            ccp_alpha=0.0,
            criterion="friedman_mse",
            init=None,
            learning_rate=0.1,
            loss="log_loss",
            max_depth=3,
            max_features=None,
            max_leaf_nodes=None,
            min_impurity_decrease=0.0,
            min_samples_leaf=1,
            min_samples_split=2,
            min_weight_fraction_leaf=0.0,
            n_estimators=100,
            n_iter_no_change=None,
            random_state=8860,
            subsample=1.0,
            tol=0.0001,
            validation_fraction=0.1,
            verbose=0,
            warm_start=False,
        ),
    ),  # Example classifier
]

# Create the pipeline for Gradient Boosting
# Write your code here

# Train the model using fit
# Write your code here

# Make predictions
# Write your code here

"""Evaluemos los resultados del modelo."""

# Print classifitacion report
clas_report = classification_report(y_test, y_pred, labels=np.unique(y_pred), digits=6)
print(clas_report)

# Plot confusion matrix
plot_confusion_matrix(confusion_matrix(y_test, y_pred), tipo_financiamiento_mapping)

"""# Pregunta 4
* *¿Cuál de los modelos consideras que es más eficiente en términos de rendimiento y por qué?*
* *Luego de evaluar los diferentes modelos, como científico de datos, ¿cuál sugerirías implementar y por qué? Justifica tu respuesta.*
* *Investiga qué otras opciones pueden ser utilizadas para enfrentar el problema de datos desbalanceados e implementa un ejemplo.*
* *Investiga qué son los modelos de ensamble e implementa un corto ejemplo.*

"""
